\noindent
\\ 
\textbf{Paper I} 
\\ \\
\noindent
N. Sebkhi et al., "A Deep Neural Network-Based Permanent Magnet Localization for Tongue Tracking," in IEEE Sensors Journal, vol. 19, no. 20, pp. 9324-9331, 15 Oct.15, 2019, doi: 10.1109/JSEN.2019.2923585.

\noindent
\textbf{Keywords} articulograph, biomedical device, deep learning, machine learning, magnetometers, motion tracking, neural network, nonlinear optimization, permanent magnet localization, speech, tongue tracking, wearable
\\ \\

\noindent
\textbf{Abstract} Permanent magnet localization (PML) is a nascent method of wireless motion tracking that can estimate the 5D state (3D position and 2D orientation) of a cylindrical magnet from its magnetic field. PML is well suited for applications where a wireless tracking is crucial, and in particular for tongue motion which opens up many new interesting applications. To allow its usage outside of a research lab, our tongue tracking system relies on the PML to avoid impeding tongue's natural motion due to its wireless tracking method and ensure safe use inside the mouth. Our tracking module is embedded in a headset to be portable and simple to use while being affordable by relying on the mass-produced components. The classical implementation of PML has many shortcomings that limit its practicality for realworld applications because it is computationally intensive due to its iterative algorithm, subject to local minimum convergence issues, and sensitive to its initial state and calibration parameters. Additionally, its physical model is an approximation that is only valid in limited tracking conditions. In this paper, we investigated the potential of deep learning to create a PML model for tongue tracking by training a feedforward neural network (3 layers, 100 neurons per layer) on a dataset composed of ~1.7 million states spanning a volume of 10 × 10 × 10 cm 3 . Our PML was validated on 337000 states in a 4 × 6 × 7 cm 3 volume and tested on 100000 samples emulating a more natural tongue motion with curves and twists. A fully automated 5D positional stage was engineered by our team to collect these large datasets of the 5D magnet states. Our PML prediction, limited to the magnet's 3D positions in this paper, reaches positional errors of 1.4 mm (median) and 1.8 mm (Q3).\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Magnetic Sensors \\ \\
\end{itemize}


\noindent
\textbf{Paper III} 
\\ \\
\noindent
Y. Nam, B. Koo, A. Cichocki and S. Choi, "Glossokinetic Potentials for a Tongue-Machine Interface: How Can We Trace Tongue Movements with Electrodes?," in IEEE Systems, Man, and Cybernetics Magazine, vol. 2, no. 1, pp. 6-13, Jan. 2016, doi: 10.1109/MSMC.2015.2490674.\\ \\

\noindent
\textbf{Keywords} Electroencephalography, Electric potential, Electric variables measurement, Motion measurement, Electrodes, Speech recognition, Kinetic theory
\\ \\

\noindent
\textbf{Abstract} Glossokinetic potentials (GKPs) refer to electrical responses involving tongue movements that are measured at electrodes placed on the scalp when the tip of the tongue touches tissue inside the mouth. GKP has been considered an electroencephalography (EEG) artifact that is removed to minimize the interference with signals from a cerebral region for successful EEG analysis. In this article, we emphasize a different side of GKP where we analyze its spatial patterns to trace tongue movements developing tongue-machine interfaces. We begin with a brief overview of GKP and its spatial patterns and then describe its potential applications to man-machine interfaces. First, we describe the spatial pattern of GKP for horizontal tongue movements and explain how it can be utilized to identify the position of the tongue. We also introduce a tongue-rudder system where this technique enables smooth control of an electric wheelchair. Then we describe GKP patterns for vertical and frontal tongue movements that are closely related to speech production. Based on this pattern, we discuss its application to silent speech recognition, which allows speech communication without using a sound.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Electrodes \\ \\
\end{itemize}

\noindent
\textbf{Paper IV} 
\\ \\
\noindent
Lotte N. S. Andreasen Struijk, Eugen R. Lontis, Michael Gaihede, Hector A. Caltenco, Morten Enemark Lund, Henrik Schioeler & Bo Bentsen (2017) Development and functional demonstration of a wireless intraoral inductive tongue computer interface for severely disabled persons, Disability and Rehabilitation: Assistive Technology, 12:6, 631-640, DOI: 10.1080/17483107.2016.1217084\\ \\

\noindent
\textbf{Keywords} Tongue computer interfaces, environmental control, neuro rehabilitation, tetraplegia
\\ \\

\noindent
\textbf{Abstract} Purpose: Individuals with tetraplegia depend on alternative interfaces in order to control computers and other electronic equipment. Current interfaces are often limited in the number of available control commands, and may compromise the social identity of an individual due to their undesirable appearance. The purpose of this study was to implement an alternative computer interface, which was fully embedded into the oral cavity and which provided multiple control commands.

Methods: The development of a wireless, intraoral, inductive tongue computer was described. The interface encompassed a 10-key keypad area and a mouse pad area. This system was embedded wirelessly into the oral cavity of the user. The functionality of the system was demonstrated in two tetraplegic individuals and two able-bodied individuals

Results: The system was invisible during use and allowed the user to type on a computer using either the keypad area or the mouse pad. The maximal typing rate was 1.8 s for repetitively typing a correct character with the keypad area and 1.4 s for repetitively typing a correct character with the mouse pad area.

Conclusion: The results suggest that this inductive tongue computer interface provides an esthetically acceptable and functionally efficient environmental control for a severely disabled user.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Inductive Sensors \\ \\
\end{itemize}


\noindent
\textbf{Paper V} 
\\ \\
\noindent
J. Woo et al., "A Sparse Non-Negative Matrix Factorization Framework for Identifying Functional Units of Tongue Behavior From MRI," in IEEE Transactions on Medical Imaging, vol. 38, no. 3, pp. 730-740, March 2019, doi: 10.1109/TMI.2018.2870939.\\ \\

\noindent
\textbf{Keywords} Tongue segmentation, tongue diagnosis, traditional Chinese medicine, TongueNet,
deep learning
\\ \\

\noindent
\textbf{Abstract} Muscle coordination patterns of lingual behaviors are synergies generated by deforming local muscle groups in a variety of ways. Functional units are functional muscle groups of local structural elements within the tongue that compress, expand, and move in a cohesive and consistent manner. Identifying the functional units using tagged-magnetic resonance imaging (MRI) sheds light on the mechanisms of normal and pathological muscle coordination patterns, yielding improvement in surgical planning, treatment, or rehabilitation procedures. In this paper, to mine this information, we propose a matrix factorization and probabilistic graphical model framework to produce building blocks and their associated weighting map using motion quantities extracted from tagged-MRI. Our tagged-MRI imaging and accurate voxel-level tracking provide previously unavailable internal tongue motion patterns, thus revealing the inner workings of the tongue during speech or other lingual behaviors. We then employ spectral clustering on the weighting map to identify the cohesive regions defined by the tongue motion that may involve multiple or undocumented regions. To evaluate our method, we perform a series of experiments. We first use two-dimensional images and synthetic data to demonstrate the accuracy of our method. We then use three-dimensional synthetic and in vivo tongue motion data using protrusion and simple speech tasks to identify subject-specific and data-driven functional units of the tongue in localized regions.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item MRI Sensors \\ \\
\end{itemize}

\noindent
\textbf{Paper VI} 
\\ \\
\noindent
B. Jiang, J. Kim and H. Park, "A New Approach of Minimizing Midas Touch Problem for a Tracer-Free Tongue-Controlled Assistive Technology," in IEEE Sensors Journal, doi: 10.1109/JSEN.2020.3013858.\\ \\

\noindent
\textbf{Keywords} assistive technology, tongue-controlled, intraoral, tracer-free, optical sensor, Midas touch problem
\\ \\

\noindent
\textbf{Abstract} Tongue-controlled assistive technologies (ATs) have a great potential to improve the quality of life for people with severe disabilities. Current tongue-controlled ATs, however, require either a tracer to be attached on the tongue or a complicated gesture/mechanism to avoid issuing commands unintendedly during routine intraoral events (e.g., tongue resting, swallowing, and speaking), which poses a high entry barrier to potential users. In this study, we propose a tracer-free tongue-controlled AT using four optical distance sensors and one ambient light sensor with a multi-sensor data fusion algorithm to distinguish the intended tongue contacts from the unintended ones. Eleven healthy human subjects participated in the study to evaluate the performance using the proposed technology in masking unintendedly-issued commands and navigating a mouse cursor on a computer. The proposed multi-sensor data fusion algorithm could mask 100\%, 86.99\%, and 99.50\% of unintendedly-issued commands during tongue resting, swallowing, and speaking, respectively. The efficacy of the tongue commands in mouse navigation was evaluated by command-issuing task and maze-navigation task. The participants could issue tongue commands in a random order with 97.16±10.94\% of accuracy. The completion time and the sum of deviation of the maze-navigation task were 31.36±2.74 s and 9.38±0.85 pixel2, respectively. The subjects’ performance on computer-access tasks, using the proposed tracer-free tongue-controlled technology, is at the level of the performance measured with existing tracer-based tongue-controlled technologies, while securing robustness in masking commands, unintendedly issued during routine intraoral events. Results of this preliminary study suggest that the tracer-free implementation is feasible option for the tongue-controlled ATs.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Optical Distance Sensors \\ \\
\end{itemize}




\subsection{Eye Trackers}
\noindent
\\ 
\textbf{Paper I} 
\\ \\
\noindent
M. A. Eid, N. Giakoumidis and A. El Saddik, "A Novel Eye-Gaze-Controlled Wheelchair System for Navigating Unknown Environments: Case Study With a Person With ALS," in IEEE Access, vol. 4, pp. 558-573, 2016, doi: 10.1109/ACCESS.2016.2520093.\\ \\

\noindent
\textbf{Keywords} Eye gaze tracking, eye gaze calibration, wheelchair control system, grid-based graphical user interface,unknown environment tracking\\ \\

\noindent
\textbf{Abstract} Thanks to advances in electric wheelchair design, persons with motor impairments due to diseases, such as amyotrophic lateral sclerosis (ALS), have tools to become more independent and mobile. However, an electric wheelchair generally requires considerable skill to learn how to use and operate. Moreover, some persons with motor disabilities cannot drive an electric wheelchair manually (even with a joystick), because they lack the physical ability to control their hand movement (such is the case with people with ALS). In this paper, we propose a novel system that enables a person with motor disability to control a wheelchair via eye-gaze and to provide a continuous, real-time navigation in unknown environments. The system comprises a Permobile M400 wheelchair, eye tracking glasses, a depth camera to capture the geometry of the ambient space, a set of ultrasound and infrared sensors to detect obstacles with low proximity that are out of the field of view for the depth camera, a laptop placed on a flexible mount for maximized comfort, and a safety off switch to turn off the system whenever needed. First, a novel algorithm is proposed to support continuous, real-time target identification, path planning, and navigation in unknown environments. Second, the system utilizes a novel N-cell grid-based graphical user interface that adapts to input/output interfaces specifications. Third, a calibration method for the eye tracking system is implemented to minimize the calibration overheads. A case study with a person with ALS is presented, and interesting findings are discussed. The participant showed improved performance in terms of calibration time, task completion time, and navigation speed for a navigation trips between office, dining room, and bedroom. Furthermore, debriefing the caregiver has also shown promising results: the participant enjoyed higher level of confidence driving the wheelchair and experienced no collisions through all the experiment.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item EOG Sensor \\ \\
\end{itemize}


\noindent
\textbf{Paper II} 
\\ \\
\noindent
C. Lee, B. Bai, Q. Song, Z. Wang and G. Li, "Microwave Resonator for Eye Tracking," in IEEE Transactions on Microwave Theory and Techniques, vol. 67, no. 12, pp. 5417-5428, Dec. 2019, doi: 10.1109/TMTT.2019.2947683.\\ \\


\noindent
\textbf{Keywords} Differential mode, eye tracking, microwave planar resonator, virtual reality (VR)/augmented reality (AR) wearable device
\\ \\

\noindent
\textbf{Abstract} This article presents a novel microwave resonator for eye tracking. The system is implemented using coplanar waveguide technology and consists of two identical configurations with a pair of mirror-symmetrical open complementary split-ring resonators. These two sensors are combined with a wearable device placed on the eye and glass frame corners. The proposed evaluation uses two uncoupled and separate reflection magnitudes for differential cross modes. Each sensor has two resonance frequencies, fL and fH, to detect horizontal and vertical eye sweeping, respectively. As the user's focus sweeps left to right or up and down with the wearable device, the asymmetrical conditions of the eyelid and cornea enhance the variation in differential reflection magnitude. The paired sensors provide information for tracking eye action. By applying the resonator perturbation theory, the eyelid and eye cornea locations above the planar resonator location were analyzed using the equivalent permittivity and conductance loss relationship, respectively. The experimental results indicate that the resonator tracker can detect relative changes in eye gaze from 50° to 130° horizontally (average 8.81-dB variation) and from 40° to 140° vertically (average 9.43-dB variation). These eye sweeping directions, thus, connect at 90°. Initial calculations reveal that the optimal eye tracking function is achieved and that the tracking method is suitable for individual users.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item open complementary split-ring resonator (OCSRR) sensor \\ \\
\end{itemize}

\noindent
\textbf{Paper III} 
\\ \\
\noindent
P. Salunkhe and A. R. Patil, "A device controlled using eye movement," 2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT), Chennai, 2016, pp. 732-735, doi: 10.1109/ICEEOT.2016.7754779.\\ \\


\noindent
\textbf{Keywords}Eye tracking, Hough Transform, Eye-blinking detection, Image processing\\ \\

\noindent
\textbf{Abstract} Research carried out in this technical paper suggests an application of the widely implemented eye tracking techniques. Traditionally, HCI uses mouse, keyboard as an input devices but this paper presents hand free interface between computer and human. Here providing a novel idea to control computer mouse cursor using human eyes movement. It controls mouse moving by automatically affecting the position where eyesight focused. The project mainly comprises of three sections namely Image Capture, Image Processing, and Cursor Control. After capturing image from webcam, the shape of pupil is recognized using Hough Transform and the center coordinate help to determine the exact point on the screen where the user is looking also coordinate will instruct the computer mouse to move specific location. This system is very helpful for solving the HMI problems of the disabled so that it can provide a way for them to communicate with the outside world.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Camera \\ \\
\end{itemize}


\noindent
\textbf{Paper IV} 
\\ \\
\noindent
Z. Hossain, M. M. H. Shuvo and P. Sarker, "Hardware and software implementation of real time electrooculogram (EOG) acquisition system to control computer cursor with eyeball movement," 2017 4th International Conference on Advances in Electrical Engineering (ICAEE), Dhaka, 2017, pp. 132-137, doi: 10.1109/ICAEE.2017.8255341.\\ \\


\noindent
\textbf{Keywords}Electrooculogram (EOG), Human Computer Interface (HCI), Cursor Movement, Support Vector Machine(SVM)\\ \\

\noindent
\textbf{Abstract} Human computer interface (HCI) is an emerging technology of neuroscience and artificial intelligence. Development of HCI system using bio signal e.g. Electrooculogram (EOG), Electromyogram (EMG), Electroencephalogram (EEG), Functional near-infrared spectroscopy (fNIRS) etc. are attracted more and more attention of researchers all over the world in recent years because through this it is possible to get acquainted with advanced technologies of artificial intelligence. This paper presents the design and implementation of a fully functional Electrooculogram (EOG) based human computer interface. In this work we have designed and implemented necessary hardware and software for EOG signal acquisition along with controlling hardware such as wheelchair, robotic arm, mobile robot etc., and move computer mouse cursor simultaneously using EOG signal. This interface has three portion: EOG signal acquisition and amplification, analog to digital conversion, and real time hardware and mouse cursor movement. Eye movement is detected by measuring potential difference between cornea and retina using five Ag-Agcl disposable electrodes. Frequency range of EOG signal is considered as 0.3 to 15Hz, so this frequency range is taken using an active high and low pass filter so that accurate EOG signal can be achieved. The analog output of the EOG signal from filter is converted into digital signal by using an Arduino. Arduino serialize the EOG data for calibration and provides a threshold reference point which is used for controlling Hardware. The Classification module e.g. Support Vector machine (SVM) and Linear Discriminant Analysis (LDA) classify live data with respect to the horizontal and vertical data. This works as a binary classifier and choose optimal hyper-plane between two variables. According to each update on the eye position, cursor automatically accelerated in particular direction. PyMouse module in python is used for this task. Eye gesture based Hardware like robot, wheelchair.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Camera \\ \\
\end{itemize}

\noindent
\textbf{Paper V} 
\\ \\
\noindent
D. Saravanakumar, R. Vishnupriya and M. R. Reddy, "A Novel EOG based Synchronous and Asynchronous Visual Keyboard System," 2019 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), Chicago, IL, USA, 2019, pp. 1-4, doi: 10.1109/BHI.2019.8834621.\\ \\


\noindent
\textbf{Keywords}Electrooculogram (EOG), Human Computer Interface (HCI), Asynchronous and Synchronous Visual Keyboard System\\ \\

\noindent
\textbf{Abstract}In this study, a high performance electrooculogram (EOG) based synchronous and asynchronous visual keyboard system is designed with large number of targets. The proposed system overcomes the limitations in the conventional EEG based visual keyboard system which has to compromise with either accuracy or typing speed of the system. In this study, we first proposed a synchronous visual keyboard system with 36 targets which includes alphabets, numbers and space. Later, we proposed an asynchronous visual keyboard system where the user can have control on the system. Ten subjects were taken for evaluating the performance of the proposed keyboard systems. A cue guided online experimental procedure was conducted on all the subjects in synchronous and asynchronous case. The average classification accuracy of 94.2\% and 98.79\% were obtained in synchronous and asynchronous cases respectively. Four subjects show 100\% accuracy in the asynchronous case with average information transfer rate (ITR) of 66.71 bits/minute. This result shows that the developed system outperforms conventional keyboard system.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item EOG Sensor \\ \\
\end{itemize}

\noindent
\textbf{Paper VI} 
\\ \\
\noindent
M. Nann et al., "Restoring Activities of Daily Living Using an EEG/EOG-Controlled Semiautonomous and Mobile Whole-Arm Exoskeleton in Chronic Stroke," in IEEE Systems Journal, doi: 10.1109/JSYST.2020.3021485.\\ \\


\noindent
\textbf{Keywords}Activities of daily living (ADL), brain-computer interface (BCI), brain-machine interface (BMI), chronic stroke, electroencephalography (EEG), electrooculography (EOG), exoskeletons, hemiparesis , rehabilitation robotics ,sensorimotor rhythms , shared control\\ \\

\noindent
\textbf{Abstract}Stroke survivors with chronic paralysis often have difficulties to perform various activities of daily living (ADLs), such as preparing a meal or eating and drinking independently. Recently, it was shown that a brain/neural hand exoskeleton can restore hand and finger function, but many stroke survivors suffer from motor deficits affecting their whole upper limb. Therefore, novel hybrid electroencephalography/electrooculography (EEG/EOG)-based brain/neural control paradigms were developed for guiding a whole-arm exoskeleton. It was unclear, however, whether hemiplegic stroke survivors are able to reliably use such brain/neural-controlled device. Here, we tested feasibility, safety, and user-friendliness of EEG/EOG-based brain/neural robotic control across five hemiplegic stroke survivors engaging in a drinking task that consisted of several subtasks (e.g., reaching, grasping, manipulating, and drinking). Reliability was assumed when at least 75\% of subtasks were initialized within 3 s. Fluent control was assumed if average “time to initialize” each subtask ranged below 3 s. System's safety and user-friendliness were rated using Likert-scales. All chronic stroke patients were able to operate the system reliably and fluently. No undesired side effects were reported. Four participants rated the system as very user-friendly. These results show that chronic stroke survivors are capable of using an EEG/EOG-controlled semiautonomous whole-arm exoskeleton restoring ADLs.
\\ \\


\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item EOG Sensor \\ \\
\end{itemize}