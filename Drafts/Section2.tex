\section{Augmentative and Assistive Communication Systems}
\lipsum[1]

\subsection{Voice/Speech Recognition}
\noindent
\\ 
\textbf{Paper I} 
\\ \\
\noindent
Lee, S., Kim, J., Yun, I. et al. An ultrathin conformable vibration-responsive electronic skin for quantitative vocal recognition. Nat Commun 10, 2468 (2019). https://doi.org/10.1038/s41467-019-10465-w

\noindent
\textbf{Keywords} Electrical and electronic engineering, vocal recognition, Sensors and biosensors
\\ \\

\noindent
\textbf{Abstract} Flexible and skin-attachable vibration sensors have been studied for use as wearable voice-recognition electronics. However, the development of vibration sensors to recognize the human voice accurately with a flat frequency response, a high sensitivity, and a flexible/conformable form factor has proved a major challenge. Here, we present an ultrathin, conformable, and vibration-responsive electronic skin that detects skin acceleration, which is highly and linearly correlated with voice pressure. This device consists of a crosslinked ultrathin polymer film and a hole-patterned diaphragm structure, and senses voices quantitatively with an outstanding sensitivity of 5.5 V Pa−1 over the voice frequency range. Moreover, this ultrathin device (<5 μm) exhibits superior skin conformity, which enables exact voice recognition because it eliminates vibrational distortion on rough and curved skin surfaces. Our device is suitable for several promising voice-recognition applications, such as security authentication, remote control systems and vocal healthcare.

\\ \\

\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item skin-attachable vibration sensors \\ \\
\end{itemize}


\subsection{Language Disorders}
\noindent
\\ 
\textbf{Paper I} 
\\ \\
\noindent
N. Sebkhi, N. M. Santus, A. Bhavsar, S. Siahpoushan and O. Inan, "Evaluation of a Wireless Tongue Tracking System on the Identification of Phoneme Landmarks," in IEEE Transactions on Biomedical Engineering, doi: 10.1109/TBME.2020.3023284.

\noindent
\textbf{Keywords} articulograph, machine learning; permanent magnet localization, phoneme, speech sound disorder, tongue tracking
\\ \\

\noindent
\textbf{Abstract} Objective: Evaluate the accuracy of a tongue tracking system based on the localization of a permanent magnet to generate a baseline of phoneme landmarks. The positional variability of the landmarks provides an indirect measure of the tracking errors to estimate the position of a small tracer attached on the tongue. The creation of a subject-independent (universal) baseline was also attempted for the first time. Method: 2,500 tongue trajectories were collected from 10 subjects tasked to utter 10 repetitions of 25 phonemes. A landmark was identified from each tongue trajectory, and tracking errors were calculated by comparing the distance of each repetition landmark to a final landmark set as their mean position. Results: In the subject-dependent baseline, the tracking errors were found to be generally consistent across all phonemes and subjects, with less than 25\% of the errors reported to be greater than 5.8 mm (median: 3.9 mm). However, the inter-subject variability showed that current limitations of our system resulted in appreciable errors (median: 55 mm, Q3: 65 mm). Conclusion: The tracking errors reported in the subject-dependent case demonstrated the potential of our system to generate a baseline of phoneme landmarks. We have identified areas of improvement that will reduce the gap between the subject-dependent and universal baseline, while lowering tracking errors to be comparable to the gold standard. Significance: Creating a baseline of phoneme landmarks can help people affected by speech sound disorders to improve their intelligibility using visual feedback that guides their tongue placement to the proper position.\\ \\

\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item Magnetic Sensors \\ \\
\end{itemize}

\\ 
\textbf{Paper II} 
\\ \\
\noindent
S. Stone and P. Birkholz, "Angle Correction in Optopalatographic Tongue Distance Measurements," in IEEE Sensors Journal, vol. 17, no. 2, pp. 459-468, 15 Jan.15, 2017, doi: 10.1109/JSEN.2016.2630742.

\noindent
\textbf{Keywords} Articulometry, stomatography, optopalatography, optical distance sensing, sub-surface scattering, sensor, simulation.
\\ \\

\noindent
\textbf{Abstract} Optopalatography (OPG) is an optical technique to measure the distance between the hard palate and the tongue surface inside the mouth during articulation. The conventional way to measure this distance is only accurate for the special case that the tongue surface is oriented perpendicular to the optical axes of the optical sensors. It introduces an error in the much more common case that the tongue is angled with respect to the sensor axes. In this paper, we present a technique to compensate that error by using a newly devised complete model of light propagation for arbitrary source-reflector-detector setups that considers the complex reflective properties of the tongue surface due to sub-surface scattering. Optimal parameters for our model are found by fitting it to a data set of real-world measurements. For any given arrangement of sensors, simulations of this model can be used to obtain optimal coefficients for a distance error correction term. This term reduces the mean distance error of in vivo data recorded with our electrooptical stomatography (EOS, a multi-modal technique that includes OPG) hardware prototype from 7.38\% to 2.25\% and the standard deviation from 2.79\% to 1.9\%. The results are used to significantly improve the precision and accuracy of the distance measurements in real-time EOS recordings and example tongue contours of five German vowels are presented.


\\ \\

\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item electrooptical stomatography (EOS)\\ \\
\end{itemize}

\\ 
\textbf{Paper III} 
\\ \\
\noindent
Herff Christian, Diener Lorenz, Angrick Miguel, Mugler Emily, Tate Matthew C., Goldrick Matthew A, Krusienski Dean J, Slutzky Marc W, Schultz Tanja, "Generating Natural, Intelligible Speech From Brain Activity in Motor, Premotor, and Inferior Frontal Cortices," in Frontiers in Neuroscience, vol. 13, pp. 1267, 2019, doi: 10.3389/fnins.2019.01267.

\noindent
\textbf{Keywords} Emg acquisition system, emg processing, electromyography sensors, myoelectric control, myoelectric signals\\ \\

\noindent
\textbf{Abstract} Neural interfaces that directly produce intelligible speech from brain activity would allow people with severe impairment from neurological disorders to communicate more naturally. Here, we record neural population activity in motor, premotor and inferior frontal cortices during speech production using electrocorticography (ECoG) and show that ECoG signals alone can be used to generate intelligible speech output that can preserve conversational cues. To produce speech directly from neural data, we adapted a method from the field of speech synthesis called unit selection, in which units of speech are concatenated to form audible output. In our approach, which we call Brain-To-Speech, we chose subsequent units of speech based on the measured ECoG activity to generate audio waveforms directly from the neural recordings. Brain-To-Speech employed the user's own voice to generate speech that sounded very natural and included features such as prosody and accentuation. By investigating the brain areas involved in speech production separately, we found that speech motor cortex provided more information for the reconstruction process than the other cortical areas.
\\ \\

\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item ECoG \\ \\
\end{itemize}



\\ 
\textbf{Paper IV} 
\\ \\
\noindent
Rameau, A. Pilot study for a novel and personalized voice restoration device for patients with laryngectomy. Head & Neck. 2020; 42: 839– 845. https://doi.org/10.1002/hed.26057

\noindent
\textbf{Keywords} laryngectomy, silent speech, voice rehabilitation
\\ \\

\noindent
\textbf{Abstract} The main modalities for voice restoration after laryngectomy are the electrolarynx, and the tracheoesophageal puncture  All have limitations and new technologies may offer innovative alternatives via silent speech. To describe a novel and personalized method of voice restoration using machine learning applied to electromyographic signal from articulatory muscles for the recognition of silent speech in a patient with total laryngectomy. Surface electromyographic (sEMG) signals of articulatory muscles were recorded from the face and neck of a patient with total laryngectomy who was articulating words silently. These sEMG signals were then used for automatic speech recognition via machine learning. Sensor placement was tailored to the patient's unique anatomy, following radiation and surgery. A personalized wearable mask covering the sensors was designed using 3D scanning and 3D printing. Using seven sEMG sensors on the patient's face and neck and two grounding electrodes, we recorded EMG data while he was mouthing “Tedd” and “Ed.” With data from 75 utterances for each of these words, we discriminated the sEMG signal with 86.4\% accuracy using an XGBoost machine‐learning model. This pilot study demonstrates the feasibility of sEMG‐based alaryngeal speech recognition, using tailored sensor placement and a personalized wearable device. Further refinement of this approach could allow translation of silently articulated speech into a synthesized voiced speech via portable devices.\\ \\

\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item sEMG surface electrode \\ \\
\end{itemize}

\\ 
\textbf{Paper V} 
\\ \\
\noindent
P. Birkholz, S. Stone, K. Wolf and D. Plettemeier, "Non-Invasive Silent Phoneme Recognition Using Microwave Signals," in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 12, pp. 2404-2411, Dec. 2018, doi: 10.1109/TASLP.2018.2865609.

\noindent
\textbf{Keywords} Speech recognition, Antenna measurements, Sensors, Radar antennas, Speech processing, Speech synthesis\\ \\

\noindent
\textbf{Abstract} Besides the recognition of audible speech, there is currently an increasing interest in the recognition of silent speech, which has a range of novel applications. A major obstacle for a wide spread of silent-speech technology is the lack of measurement methods for speech movements that are convenient, non-invasive, portable, and robust at the same time. Therefore, as an alternative to established methods, we examined to what extent different phonemes can be discriminated from the electromagnetic transmission and reflection properties of the vocal tract. To this end, we attached two Vivaldi antennas on the cheek and below the chin of two subjects. While the subjects produced 25 phonemes in multiple phonetic contexts each, we measured the electromagnetic transmission spectra from one antenna to the other, and the reflection spectra for each antenna (radar), in a frequency band from 2-12 GHz. Two classification methods (k-nearest neighbors and linear discriminant analysis) were trained to predict the phoneme identity from the spectral data. With linear discriminant analysis, cross-validated phoneme recognition rates of 93\% and 85\% were achieved for the two subjects. Although these results are speaker- and session-dependent, they suggest that electromagnetic transmission and reflection measurements of the vocal tract have great potential for future silent-speech interfaces.\\ \\

\noindent
\textbf{Sensors in use:}
\begin{itemize}
    \item flat foil antennas \\ \\
\end{itemize}


\subsection{Sensors in Use}
\subsubsection{Magnetic Sensors}
\subsubsection{Inertia Sensors}
\subsubsection{Mircophone}